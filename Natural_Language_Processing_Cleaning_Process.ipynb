{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Data Preparation: \n",
    "### Clean of the Data:\n",
    "download the txt file from http://www.gutenberg.org/cache/epub/5200/pg5200.txt and remove the header and footer and renamed as metamorphosis_clean.txt file.\n",
    "\n",
    "#### We will see by only :\n",
    "i) Cleaning Manually and with NLTK &\n",
    "\n",
    "ii) Prepare text data with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï»¿One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment.', 'His', 'many', 'legs,', 'pitifully', 'thin']\n"
     ]
    }
   ],
   "source": [
    "#i)Clean Manually and with NLTK\n",
    "# Load the Data\n",
    "filename='metamorphosis_clean.txt'\n",
    "file=open(filename,'rt')\n",
    "text=file.read()\n",
    "file.close()\n",
    "# split into words by white space \"\"(space),new lines,tabs and more\n",
    "words=text.split()\n",
    "print(words[:70]) # display first 70 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs']\n"
     ]
    }
   ],
   "source": [
    "# other way\n",
    "# Load the Data\n",
    "import re  # regex model(re)\n",
    "filename='metamorphosis_clean.txt'\n",
    "file=open(filename,'rt')\n",
    "text=file.read()\n",
    "file.close()\n",
    "# split into words by white space \"\"(space),new lines,tabs and more\n",
    "words=re.split(r'\\W+',text)\n",
    "print(words[:70]) # display first 70 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# print punctation\n",
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs']\n"
     ]
    }
   ],
   "source": [
    "### Remove the Punctuation\n",
    "#prepare regex for char filtering\n",
    "re_punc=re.compile('[%s]'% re.escape(string.punctuation))\n",
    "#remove punctuation from each word\n",
    "stripped=[re_punc.sub('',w) for w in words] # use of sub() to replace punctuation character with \n",
    "# nothing.\n",
    "print(stripped[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs']\n"
     ]
    }
   ],
   "source": [
    "# sometimes the text data may contain non-printable character. use similar approach to filter out\n",
    "# all non-printable characters by selecting the inverse of the string.printable constant.\n",
    "#prepare regex for char filtering\n",
    "re_print=re.compile('[^%s]'% re.escape(string.printable))\n",
    "\n",
    "result=[re_print.sub('',w) for w in words] # use of sub() to replace punctuation character with \n",
    "# nothing.\n",
    "print(result[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'one', 'morning', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'he', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'the', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'his', 'many', 'legs']\n"
     ]
    }
   ],
   "source": [
    "# NOrmalizing Case\n",
    "# converting all words to lowecase. This means that vocabulary will shrink in size,but\n",
    "# some distinctions are lost(Apple company vs apple fruits)\n",
    "words= [word.lower() for word in words]\n",
    "print(words[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and cleaning with NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# command to instal the nltk library(one time)\n",
    "#import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿One morning, when Gregor Samsa woke from troubled dreams, he found\n",
      "himself transformed in his bed into a horrible vermin.\n"
     ]
    }
   ],
   "source": [
    "# Split text into sentence:\n",
    "from nltk import sent_tokenize\n",
    "#load the data\n",
    "filename='metamorphosis_clean.txt'\n",
    "file=open(filename,'rt')\n",
    "text=file.read()\n",
    "file.close()\n",
    "# split into the sentence\n",
    "sentence=sent_tokenize(text)\n",
    "print(sentence[0])  # print the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', '»', '¿One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', ',', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', '.', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide']\n"
     ]
    }
   ],
   "source": [
    "# Split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens=word_tokenize(text)\n",
    "print(tokens[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide', 'off', 'any', 'moment', 'His', 'many', 'legs', 'pitifully', 'thin', 'compared']\n"
     ]
    }
   ],
   "source": [
    "# Filter out punctuation: filter out standalone punctuation. isalpha() function is used.\n",
    "words=[word for word in tokens if word.isalpha()]\n",
    "print(words[:70])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Filter out stop words ( and pipeline): Stop words do not contribute to the deeper meaning of the\n",
    "# phrase. Like the,a, and, is , etc\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')  # we are only interested in english language.\n",
    "print(stop_words)  # shows full list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples', 'lay', 'spread', 'table', 'samsa', 'travelling', 'salesman', 'hung', 'picture', 'recently', 'cut', 'illustrated', 'magazine']\n"
     ]
    }
   ],
   "source": [
    "# Filter out stop words ( and pipeline): Stop words do not contribute to the deeper meaning of the\n",
    "# phrase. Like the,a, and, is , etc\n",
    "\n",
    "# PIPELINE: PROCESS:\n",
    "    # import the necessary library\n",
    "import string\n",
    "import re   # regex\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "    # load dataset\n",
    "filename='metamorphosis_clean.txt'\n",
    "file=open(filename,'rt')\n",
    "text=file.read()\n",
    "file.close()\n",
    "    # split into words\n",
    "tokens=word_tokenize(text)\n",
    "    # convert to lower case\n",
    "tokens= [w.lower() for w in tokens]\n",
    "    # prepare regex for char filtering\n",
    "re_punc= re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "stripped= [re_punc.sub('',w) for w in tokens]\n",
    "\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "words= [word for word in stripped if word.isalpha()]\n",
    "\n",
    "    # filter out stop words\n",
    "stop_words=set(stopwords.words('english'))\n",
    "words=[ w for w in words if not w in stop_words]\n",
    "print(words[:77])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'morn', 'gregor', 'samsa', 'woke', 'troubl', 'dream', 'found', 'transform', 'bed', 'horribl', 'vermin', 'lay', 'armourlik', 'back', 'lift', 'head', 'littl', 'could', 'see', 'brown', 'belli', 'slightli', 'dome', 'divid', 'arch', 'stiff', 'section', 'bed', 'hardli', 'abl', 'cover', 'seem', 'readi', 'slide', 'moment', 'mani', 'leg', 'piti', 'thin', 'compar', 'size', 'rest', 'wave', 'helplessli', 'look', 'happen', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'littl', 'small', 'lay', 'peac', 'four', 'familiar', 'wall', 'collect', 'textil', 'sampl', 'lay', 'spread', 'tabl', 'samsa', 'travel', 'salesman', 'hung', 'pictur', 'recent', 'cut', 'illustr', 'magazin']\n"
     ]
    }
   ],
   "source": [
    "# Stem Words:\n",
    "   # stemming refers to the process of reducing each word to its root or base. For example\n",
    "    # fishing,fished,fisher , all reduces to stem fish. It is used to focus on sense or sentiment\n",
    "    # of a document rather than deeper meaning.There are many stemming algorithms, the popular\n",
    "    # one is Porter Stemming algorithm.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter= PorterStemmer()\n",
    "stemmed=[porter.stem(word) for word in words]\n",
    "print(stemmed[:77])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition Text cleaning cosiderations:\n",
    "Since are above data is almost clean , we may encounter a raw data which will be much messy.\n",
    "Here is a shortlist of additional considerations when cleaning text:\n",
    "\n",
    " Handling large documents and large collections of text documents that do not ﬁt into memory.\n",
    "\n",
    " Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "\n",
    " Transliteration of characters from other languages into English.\n",
    "\n",
    " Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "\n",
    " Handling of domain speciﬁc words, phrases, and acronyms.\n",
    "\n",
    " Handling or removing numbers, such as dates and amounts.\n",
    "\n",
    " Locating and correcting common typos and misspellings.\n",
    "\n",
    " And much more...\n",
    "\n",
    "The list could go on. Hopefully, you can see that getting truly clean text is impossible, that we are really doing the best we can based on the time, resources, and knowledge we have. The idea of clean is really deﬁned by the speciﬁc task or concern of your project. A pro tip is to continually review your tokens after every transform. I have tried to show that in this tutorial and I hope you take that to heart. Ideally, you would save a new ﬁle after each transform so that you can spend time with all of the data in the new form. Things always jump out at you when to take the time to review your data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Prepare text data with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good ﬁrst step when working with text is to split it into words. Words are called tokens and the process of splitting text into tokens is called tokenization. Keras provides the text to word sequence() function that you can use to split text into a list of words. By default, this function automatically does 3 things:\n",
    "\n",
    " Splits words by space.\n",
    "\n",
    " Filters out punctuation.\n",
    "\n",
    " Converts text to lowercase (lower=True).\n",
    "\n",
    "You can change any of these defaults by passing arguments to the function. Below is an example of using the text to word sequence() function to split a document (in this case a simple string) into a list of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# Let take the text\n",
    "text='The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "words=text_to_word_sequence(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding with one hot\n",
    "It is popular to represent a document as a sequence of integer values, where each word in the document is represented as a unique integer. Keras provides the one hot() function that you can use to tokenize and integer encode a text document in one step. The name suggests that it will create a one hot encoding of the document, which is not the case. Instead, the function is a wrapper for the hashing trick() function described in the next section. The function returns an integer encoded version of the document. The use of a hash function means that there may be collisions and not all words will be assigned unique integer values. As with the text to word sequence() function in the previous section, the one hot() function will make the text lower case, ﬁlter out punctuation, and split words based on white space.\n",
    "\n",
    "In addition to the text, the vocabulary size (total words) must be speciﬁed. This could be the total number of words in the document or more if you intend to encode additional documents that contains additional words. The size of the vocabulary deﬁnes the hashing space from which words are hashed. By default, the hash function is used, although as we will see in the next section, alternate hash functions can be speciﬁed when calling the hashing trick() function directly.\n",
    "\n",
    "We can use the text to word sequence() function from the previous section to split the document into words and then use a set to represent only the unique words in the document. The size of this set can be used to estimate the size of the vocabulary for one document. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[1, 6, 4, 3, 2, 8, 1, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "# estimate the size of the vocabulary\n",
    "words= set(text_to_word_sequence(text))\n",
    "vocab_size=len(words)\n",
    "print(vocab_size)\n",
    "# Integer encode the document.\n",
    "result= one_hot(text,round(vocab_size*1.3)) # the vocabulary size is increased by one-third to\n",
    "# minimize collision when hashing words.\n",
    "print(result) # gives the array of integer encoded words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Encoding with hashing trick\n",
    "A limitation of integer and count base encodings is that they must maintain a vocabulary of words and their mapping to integers. An alternative to this approach is to use a one-way hash function to convert words to integers. This avoids the need to keep track of a vocabulary, which is faster and requires less memory.\n",
    "\n",
    "Keras provides the hashing trick() function that tokenizes and then integer encodes the document, just like the one hot() function. It provides more ﬂexibility, allowing you to specify the hash function as either hash (the default) or other hash functions such as the built in md5 function or your own function. Below is an example of integer encoding a document using the md5 hash function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import hashing_trick\n",
    "# Let take the text\n",
    "text='The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "words=text_to_word_sequence(text)\n",
    "# estimate the size of the vocabulary\n",
    "words= set(text_to_word_sequence(text))\n",
    "vocab_size=len(words)\n",
    "print(vocab_size)\n",
    "# Integer encode the document.\n",
    "result= hashing_trick(text,round(vocab_size*1.3),hash_function='md5') # the vocabulary size is \n",
    "# increased by one-third to minimize collision when hashing words.\n",
    "print(result) # gives the array of integer encoded words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer API\n",
    "So far we have looked at one-oﬀ convenience methods for preparing text with Keras. Keras provides a more sophisticated API for preparing text that can be ﬁt and reused to prepare multiple text documents. This may be the preferred approach for large projects. Keras provides the Tokenizer class for preparing text documents for deep learning. The Tokenizer must be constructed and then ﬁt on either raw text documents or integer encoded text documents.\n",
    "\n",
    "Once ﬁt, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:\n",
    "\n",
    " word counts: A dictionary mapping of words and their occurrence counts when the Tokenizer was ﬁt.\n",
    "\n",
    " word docs: A dictionary mapping of words and the number of documents that reach appears in.\n",
    "\n",
    " word index: A dictionary of words and their uniquely assigned integers.\n",
    "\n",
    "\n",
    " document count: A dictionary mapping and the number of documents they appear in calculated during the ﬁt.\n",
    "\n",
    "Once the Tokenizer has been ﬁt on training data, it can be used to encode documents in the train or test datasets. The texts to matrix() function on the Tokenizer can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary. This function provides a suite of standard bag-of-words model text encoding schemes that can be provided via a mode argument to the function. The modes available include:\n",
    "\n",
    " binary: Whether or not each word is present in the document. This is the default.\n",
    "\n",
    " count: The count of each word in the document.\n",
    "\n",
    " tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
    "\n",
    " freq: The frequency of each word as a ratio of words within each document.\n",
    "\n",
    "We can put all of this together with a worked example.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "\n",
      "\n",
      "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'work': 2, 'good': 1, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n",
      "\n",
      "\n",
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "# define 5 documents \n",
    "docs = ['Well done!', 'Good work', 'Great effort', 'nice work', 'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs) # which gives 4 attributes, printing these attributes as follows\n",
    "# Printing out the 4 attributes of fit_on_texts function\n",
    "print(t.word_counts)\n",
    "print('\\n')\n",
    "print(t.document_count)\n",
    "print('\\n')\n",
    "print(t.word_index) \n",
    "print('\\n')\n",
    "print(t.word_docs)\n",
    "print('\\n')\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count') \n",
    "print(encoded_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
